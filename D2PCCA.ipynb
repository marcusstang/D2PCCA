{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# D$^2$PCCA Model\n",
        "\n",
        "Code adapted from https://github.com/pyro-ppl/pyro/blob/dev/examples/dmm.py"
      ],
      "metadata": {
        "id": "HKQzUbQLgEA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CigySmf9gDXi"
      },
      "outputs": [],
      "source": [
        "!pip install pyro-ppl\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import pyro\n",
        "import pyro.poutine as poutine\n",
        "import pyro.distributions as dist\n",
        "from pyro.distributions import TransformedDistribution\n",
        "from pyro.distributions.transforms import affine_autoregressive\n",
        "from pyro.infer import (\n",
        "    SVI,\n",
        "    JitTrace_ELBO,\n",
        "    Trace_ELBO,\n",
        "    TraceEnum_ELBO,\n",
        "    TraceTMC_ELBO,\n",
        "    config_enumerate,\n",
        ")\n",
        "from pyro.optim import (\n",
        "    Adam,\n",
        "    ClippedAdam,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# p(x_t | z_t) or p(y_t | z_t)\n",
        "class Emitter_1_input(nn.Module):\n",
        "    def __init__(self, output_dim, z_dim, emission_dim):\n",
        "        super().__init__()\n",
        "        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)\n",
        "        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)\n",
        "        self.lin_hidden_to_loc = nn.Linear(emission_dim, output_dim)\n",
        "        self.lin_hidden_to_scale = nn.Linear(emission_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z_t):\n",
        "        h1 = self.relu(self.lin_z_to_hidden(z_t))\n",
        "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
        "        x_loc = self.lin_hidden_to_loc(h2)\n",
        "        x_scale = torch.exp(self.lin_hidden_to_scale(h2))\n",
        "        return x_loc, x_scale\n",
        "\n",
        "# p(x_t | z_t, z_t^1) or p(y_t | z_t, z_t^2)\n",
        "class Emitter_2_input(nn.Module):\n",
        "    def __init__(self, x_dim, z_dim, zx_dim, emission_dim_zx_x):\n",
        "        super().__init__()\n",
        "        self.lin_z_to_hidden = nn.Linear(z_dim + zx_dim, emission_dim_zx_x)\n",
        "        self.lin_hidden_to_hidden = nn.Linear(emission_dim_zx_x, emission_dim_zx_x)\n",
        "        self.lin_hidden_to_loc = nn.Linear(emission_dim_zx_x, x_dim)\n",
        "        self.lin_hidden_to_scale = nn.Linear(emission_dim_zx_x, x_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, z_t, zi_t):\n",
        "        z_combined = torch.cat((z_t, zi_t), dim=-1)\n",
        "        h1 = self.relu(self.lin_z_to_hidden(z_combined))\n",
        "        h2 = self.relu(self.lin_hidden_to_hidden(h1))\n",
        "        x_loc = self.lin_hidden_to_loc(h2)\n",
        "        x_scale = torch.exp(self.lin_hidden_to_scale(h2))\n",
        "        return x_loc, x_scale\n",
        "\n",
        "# p(z_t | z_{t-1})\n",
        "class GatedTransition(nn.Module):\n",
        "    def __init__(self, z_dim, zx_dim=0, zy_dim=0, transition_dim_z=10, transition_dim_zx=10, transition_dim_zy=10):\n",
        "        super().__init__()\n",
        "        # g_t: gating units\n",
        "        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim_z)\n",
        "        self.lin_gate_hidden_to_z = nn.Linear(transition_dim_z, z_dim)\n",
        "        # h_t: proposed mean\n",
        "        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim_z)\n",
        "        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim_z, z_dim)\n",
        "        # S(Z_{t-1}): variance\n",
        "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
        "        # MLP(z_{t-1}, I)\n",
        "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
        "        # modify the default initialization of lin_z_to_loc so that it's starts out as the identity function\n",
        "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
        "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
        "\n",
        "        if zx_dim != 0:\n",
        "            self.lin_gate_zx_to_hidden = nn.Linear(zx_dim, transition_dim_zx)\n",
        "            self.lin_gate_hidden_to_zx = nn.Linear(transition_dim_zx, zx_dim)\n",
        "            self.lin_proposed_mean_zx_to_hidden = nn.Linear(zx_dim, transition_dim_zx)\n",
        "            self.lin_proposed_mean_hidden_to_zx = nn.Linear(transition_dim_zx, zx_dim)\n",
        "            self.lin_sig_x = nn.Linear(zx_dim, zx_dim)\n",
        "            self.lin_zx_to_loc = nn.Linear(zx_dim, zx_dim)\n",
        "            self.lin_zx_to_loc.weight.data = torch.eye(zx_dim)\n",
        "            self.lin_zx_to_loc.bias.data = torch.zeros(zx_dim)\n",
        "\n",
        "        if zy_dim != 0:\n",
        "            self.lin_gate_zy_to_hidden = nn.Linear(zy_dim, transition_dim_zy)\n",
        "            self.lin_gate_hidden_to_zy = nn.Linear(transition_dim_zy, zy_dim)\n",
        "            self.lin_proposed_mean_zy_to_hidden = nn.Linear(zy_dim, transition_dim_zy)\n",
        "            self.lin_proposed_mean_hidden_to_zy = nn.Linear(transition_dim_zy, zy_dim)\n",
        "            self.lin_sig_y = nn.Linear(zy_dim, zy_dim)\n",
        "            self.lin_zy_to_loc = nn.Linear(zy_dim, zy_dim)\n",
        "            self.lin_zy_to_loc.weight.data = torch.eye(zy_dim)\n",
        "            self.lin_zy_to_loc.bias.data = torch.zeros(zy_dim)\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.zx_dim = zx_dim\n",
        "        self.zy_dim = zy_dim\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, z_t_1):\n",
        "        z_t = z_t_1[:, :self.z_dim]\n",
        "        # g_t: gating units\n",
        "        _gate = self.relu(self.lin_gate_z_to_hidden(z_t))\n",
        "        gate = self.sigmoid(self.lin_gate_hidden_to_z(_gate))\n",
        "        # h_t: proposed mean\n",
        "        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t))\n",
        "        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)\n",
        "        # loc, sacle\n",
        "        z_loc = (1 - gate) * self.lin_z_to_loc(z_t) + gate * proposed_mean\n",
        "        z_scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))\n",
        "\n",
        "        # naive model: z_t -> z_{t+1}\n",
        "        if self.zx_dim == 0 and self.zy_dim == 0:\n",
        "            return z_loc, z_scale\n",
        "\n",
        "        # cca: z_t -> z_{t+1}; zx_t -> zx_{t+1}; zy_t -> zy_{t+1}\n",
        "        # z_t_1 = [z, zx, zy]\n",
        "        if self.zx_dim != 0:\n",
        "            # zx_t -> zx_{t+1}\n",
        "            zx_t = z_t_1[:, self.z_dim:self.z_dim+self.zx_dim]\n",
        "            _gate_x = self.relu(self.lin_gate_zx_to_hidden(zx_t))\n",
        "            gate_x = self.sigmoid(self.lin_gate_hidden_to_zx(_gate_x))\n",
        "            _proposed_mean_x = self.relu(self.lin_proposed_mean_zx_to_hidden(zx_t))\n",
        "            proposed_mean_x = self.lin_proposed_mean_hidden_to_zx(_proposed_mean_x)\n",
        "            zx_loc = (1 - gate_x) * self.lin_zx_to_loc(zx_t) + gate_x * proposed_mean_x\n",
        "            zx_scale = self.softplus(self.lin_sig_x(self.relu(proposed_mean_x)))\n",
        "\n",
        "            if self.zy_dim == 0:\n",
        "                # pls: z_t -> z_{t+1}; zx_t -> zx_{t+1}\n",
        "                pls_loc = torch.cat((z_loc, zx_loc), dim=-1)\n",
        "                pls_scale = torch.cat((z_scale, zx_scale), dim=-1)\n",
        "                return pls_loc, pls_scale\n",
        "\n",
        "            # zy_t -> zy_{t+1}\n",
        "            zy_t = z_t_1[:, self.z_dim+self.zx_dim:]\n",
        "            _gate_y = self.relu(self.lin_gate_zy_to_hidden(zy_t))\n",
        "            gate_y = self.sigmoid(self.lin_gate_hidden_to_zy(_gate_y))\n",
        "            _proposed_mean_y = self.relu(self.lin_proposed_mean_zy_to_hidden(zy_t))\n",
        "            proposed_mean_y = self.lin_proposed_mean_hidden_to_zy(_proposed_mean_y)\n",
        "            zy_loc = (1 - gate_y) * self.lin_zy_to_loc(zy_t) + gate_y * proposed_mean_y\n",
        "            zy_scale = self.softplus(self.lin_sig_y(self.relu(proposed_mean_y)))\n",
        "\n",
        "            # concat\n",
        "            cca_loc = torch.cat((z_loc, zx_loc, zy_loc), dim=-1)\n",
        "            cca_scale = torch.cat((z_scale, zx_scale, zy_scale), dim=-1)\n",
        "            return cca_loc, cca_scale\n",
        "\n",
        "\n",
        "\n",
        "# q(z_t|z_{t-1}, h_t^r)\n",
        "class Combiner(nn.Module):\n",
        "    def __init__(self, z_dim, rnn_dim):\n",
        "        super().__init__()\n",
        "        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
        "        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
        "        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, z_t_1, h_rnn):\n",
        "        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)\n",
        "        loc = self.lin_hidden_to_loc(h_combined)\n",
        "        scale = self.softplus(self.lin_hidden_to_scale(h_combined))\n",
        "        return loc, scale\n",
        "\n",
        "# z: hidden chain shared by x and y.\n",
        "# z_x: hidden chain unique for x\n",
        "# z_y: hidden chain unique for y\n",
        "\n",
        "# pls: zx -> x <- z -> y\n",
        "\n",
        "class D2PCCA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        x_dim=100,                # x dimensions\n",
        "        y_dim=100,                # y dimensions\n",
        "        z_dim=100,                # z dimensions\n",
        "        zx_dim=0,                 # z_x dimensions\n",
        "        zy_dim=0,                 # z_y dimensions\n",
        "        emission_dim_z_x=100,     # hidden dimensions in emission network from z to x\n",
        "        emission_dim_z_y=100,     # hidden dimensions in emission network from z to y\n",
        "        emission_dim_zx_x=0,      # hidden dimensions in emission network from z,z_x to x\n",
        "        emission_dim_zy_y=0,      # hidden dimensions in emission network from z,z_y to y\n",
        "        transition_dim_z=100,     # hidden dimension in transition network z\n",
        "        transition_dim_zx=0,      # hidden dimension in transition network z_x\n",
        "        transition_dim_zy=0,      # hidden dimension in transition network z_y\n",
        "        rnn_dim=600,              # RNN hidden dimensions\n",
        "        num_layers=1,             # RNN layers\n",
        "        rnn_dropout_rate=0.0,     # RNN dropout rate\n",
        "        multisteps=1,\n",
        "        num_iafs=0,\n",
        "        iaf_dim=50,\n",
        "        beta_d = .1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # naive model: x <- z -> y\n",
        "        if zx_dim == 0 and zy_dim == 0:\n",
        "            self.emitter_z_x = Emitter_1_input(x_dim, z_dim, emission_dim_z_x) # z -> x\n",
        "            self.emitter_z_y = Emitter_1_input(y_dim, z_dim, emission_dim_z_y) # z -> y\n",
        "        # cca: zx -> x <- z -> y <- zy\n",
        "        if zx_dim != 0 and zy_dim != 0:\n",
        "            self.emitter_zx_x = Emitter_2_input(x_dim, z_dim, zx_dim, emission_dim_zx_x) # z, z_x -> x\n",
        "            self.emitter_zy_y = Emitter_2_input(y_dim, z_dim, zy_dim, emission_dim_zy_y) # z, z_y -> y\n",
        "        # pls: zx -> x <- z -> y\n",
        "        if zx_dim != 0 and zy_dim == 0:\n",
        "            self.emitter_zx_x = Emitter_2_input(x_dim, z_dim, zx_dim, emission_dim_zx_x) # z, z_x -> x\n",
        "            self.emitter_z_y = Emitter_1_input(y_dim, z_dim, emission_dim_z_y) # z -> y\n",
        "\n",
        "        self.trans = GatedTransition(z_dim, zx_dim, zy_dim, transition_dim_z, transition_dim_zx, transition_dim_zy) # z_t -> z_{t+1}\n",
        "        self.combiner = Combiner(z_dim + zx_dim + zy_dim, rnn_dim)\n",
        "        # dropout just takes effect on inner layers of rnn\n",
        "        rnn_dropout_rate = 0.0 if num_layers == 1 else rnn_dropout_rate\n",
        "        self.rnn = nn.RNN(\n",
        "            input_size=x_dim + y_dim,\n",
        "            hidden_size=rnn_dim,\n",
        "            nonlinearity=\"relu\",\n",
        "            batch_first=True,\n",
        "            bidirectional=False,\n",
        "            num_layers=num_layers,\n",
        "            dropout=rnn_dropout_rate,\n",
        "        )\n",
        "        # normalizing flows\n",
        "        self.iafs = [\n",
        "            affine_autoregressive(z_dim+zx_dim+zy_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)\n",
        "        ]\n",
        "        self.iafs_modules = nn.ModuleList(self.iafs)\n",
        "        # p(z_0)\n",
        "        self.z_0 = nn.Parameter(torch.zeros(z_dim + zx_dim + zy_dim))\n",
        "        # q(z_0)\n",
        "        self.z_q_0 = nn.Parameter(torch.zeros(z_dim + zx_dim + zy_dim))\n",
        "        # initial hidden state of the rnn\n",
        "        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))\n",
        "        # register attributes\n",
        "        self.z_dim = z_dim\n",
        "        self.zx_dim = zx_dim\n",
        "        self.zy_dim = zy_dim\n",
        "        self.multisteps = multisteps\n",
        "        self.beta_d = beta_d\n",
        "\n",
        "    def model(self, mini_batch_x, mini_batch_y, annealing_factor=1.0):\n",
        "        pyro.module(\"dmm\", self)\n",
        "        T_max = mini_batch_x.size(1) # T\n",
        "        batch_size = mini_batch_x.size(0) # batch size\n",
        "\n",
        "        # p(z_0)\n",
        "        z_prev = self.z_0.expand(batch_size, self.z_0.size(0)) # replicate z_0 batch_size times.\n",
        "\n",
        "        with pyro.plate(\"z_minibatch\", batch_size):\n",
        "            for t in pyro.markov(range(1, T_max + 1)):\n",
        "                # p(z_hat_t | z_hat_{t-1})\n",
        "                z_all_loc, z_all_scale = self.trans(z_prev)\n",
        "                with poutine.scale(scale=annealing_factor):\n",
        "                    z_all_t = pyro.sample(\"z_%d\" % t, dist.Normal(z_all_loc, z_all_scale).to_event(1))\n",
        "                # naive model\n",
        "                if self.zx_dim == 0 and self.zy_dim == 0:\n",
        "                    x_loc, x_scale = self.emitter_z_x(z_all_t)\n",
        "                    y_loc, y_scale = self.emitter_z_y(z_all_t)\n",
        "                # cca\n",
        "                if self.zx_dim != 0 and self.zy_dim != 0:\n",
        "                    z_t = z_all_t[:, :self.z_dim]\n",
        "                    zx_t = z_all_t[:, self.z_dim:self.z_dim+self.zx_dim]\n",
        "                    zy_t = z_all_t[:, self.z_dim+self.zx_dim:]\n",
        "                    x_loc, x_scale = self.emitter_zx_x(z_t, zx_t)\n",
        "                    y_loc, y_scale = self.emitter_zy_y(z_t, zy_t)\n",
        "                # pls\n",
        "                if self.zx_dim != 0 and self.zy_dim == 0:\n",
        "                    z_t = z_all_t[:, :self.z_dim]\n",
        "                    zx_t = z_all_t[:, self.z_dim:]\n",
        "                    x_loc, x_scale = self.emitter_zx_x(z_t, zx_t)\n",
        "                    y_loc, y_scale = self.emitter_z_y(z_t)\n",
        "                pyro.sample(\"obs_x_%d\" % t, dist.Normal(x_loc, x_scale).to_event(1), obs=mini_batch_x[:, t - 1, :])\n",
        "                pyro.sample(\"obs_y_%d\" % t, dist.Normal(y_loc, y_scale).to_event(1), obs=mini_batch_y[:, t - 1, :])\n",
        "                # update time step\n",
        "                z_prev = z_all_t\n",
        "\n",
        "    def guide(self, mini_batch_x, mini_batch_y, annealing_factor=1.0):\n",
        "        pyro.module(\"dmm\", self)\n",
        "        T_max = mini_batch_x.size(1) # T\n",
        "        batch_size = mini_batch_x.size(0) # batch size\n",
        "        # combining x and y batches\n",
        "        mini_batch_combined = torch.cat((mini_batch_x, mini_batch_y), dim=-1)\n",
        "        # expand h_0 to fit batch size\n",
        "        h_0_contig = self.h_0.expand(1, batch_size, self.rnn.hidden_size).contiguous()\n",
        "        # reverse batch\n",
        "        mini_batch_reversed = torch.flip(mini_batch_combined, dims=[1])\n",
        "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
        "        rnn_output = torch.flip(rnn_output, dims=[1])\n",
        "\n",
        "        # q(z_0)\n",
        "        z_prev = self.z_q_0.expand(batch_size, self.z_q_0.size(0))\n",
        "\n",
        "        with pyro.plate(\"z_minibatch\", batch_size):\n",
        "            for t in pyro.markov(range(1, T_max + 1)):\n",
        "                # ST-R: q(z_t | z_{t-1}, x_{t:T}, y_{t:T})\n",
        "                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])\n",
        "                if len(self.iafs) > 0:\n",
        "                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)\n",
        "                else:\n",
        "                    z_dist = dist.Normal(z_loc, z_scale)\n",
        "                with pyro.poutine.scale(scale=annealing_factor):\n",
        "                    if len(self.iafs) > 0:\n",
        "                        z_t = pyro.sample(\"z_%d\" % t, z_dist)\n",
        "                    else:\n",
        "                        z_t = pyro.sample(\"z_%d\" % t, z_dist.to_event(1))\n",
        "                # update time step\n",
        "                z_prev = z_t\n",
        "\n",
        "    # latent overshooting\n",
        "    def model_lo(self, mini_batch_x, mini_batch_y):\n",
        "        pyro.module(\"dmm\", self)\n",
        "        T_max = mini_batch_x.size(1) # T\n",
        "        d = self.multisteps # d=1 by default\n",
        "        batch_size = mini_batch_x.size(0) # batch size\n",
        "\n",
        "        # p(z_0)\n",
        "        z_0_expand = self.z_0.expand(batch_size, self.z_0.size(0))\n",
        "        z_prev = torch.zeros(batch_size, d, self.z_0.size(0))\n",
        "        z_prev = torch.cat([z_0_expand.unsqueeze(1), z_prev[:, 1:, :]], dim=1)\n",
        "        z_cur = torch.zeros(batch_size, d, self.z_0.size(0))\n",
        "\n",
        "        with pyro.plate(\"z_minibatch\", batch_size):\n",
        "            for t in pyro.markov(range(1, T_max + 1)):\n",
        "                # z_{t|t}\n",
        "                z_tt_prev = z_prev[:,0,:].squeeze()\n",
        "                z_tt_loc, z_tt_scale = self.trans(z_tt_prev)\n",
        "                z_tt_dist = dist.Normal(z_tt_loc, z_tt_scale)\n",
        "                #with pyro.poutine.scale(scale=self.beta_d):\n",
        "                z_tt = pyro.sample(f\"z_{t}_{t}\", z_tt_dist.to_event(1))\n",
        "                z_cur = torch.cat([z_tt.unsqueeze(1), z_cur[:, 1:, :]], dim=1)\n",
        "                # p(obs|z_{t|t})\n",
        "                # naive model\n",
        "                if self.zx_dim == 0 and self.zy_dim == 0:\n",
        "                    x_loc, x_scale = self.emitter_z_x(z_tt)\n",
        "                    y_loc, y_scale = self.emitter_z_y(z_tt)\n",
        "                # cca\n",
        "                if self.zx_dim != 0 and self.zy_dim != 0:\n",
        "                    z_t = z_tt[:, :self.z_dim]\n",
        "                    zx_t = z_tt[:, self.z_dim:self.z_dim+self.zx_dim]\n",
        "                    zy_t = z_tt[:, self.z_dim+self.zx_dim:]\n",
        "                    x_loc, x_scale = self.emitter_zx_x(z_t, zx_t)\n",
        "                    y_loc, y_scale = self.emitter_zy_y(z_t, zy_t)\n",
        "                # pls\n",
        "                if self.zx_dim != 0 and self.zy_dim == 0:\n",
        "                    z_t = z_tt[:, :self.z_dim]\n",
        "                    zx_t = z_tt[:, self.z_dim:]\n",
        "                    x_loc, x_scale = self.emitter_zx_x(z_t, zx_t)\n",
        "                    y_loc, y_scale = self.emitter_z_y(z_t)\n",
        "                # multiplication factor\n",
        "                #mf = d if t >= d else t\n",
        "                #with pyro.poutine.scale(scale=1):\n",
        "                pyro.sample(\"obs_x_%d\" % t, dist.Normal(x_loc, x_scale).to_event(1), obs=mini_batch_x[:, t - 1, :])\n",
        "                pyro.sample(\"obs_y_%d\" % t, dist.Normal(y_loc, y_scale).to_event(1), obs=mini_batch_y[:, t - 1, :])\n",
        "                # z_{t|t-1}\n",
        "\n",
        "                z_tt1 = pyro.sample(f\"z_{t}_{t-1}_r\", z_tt_dist.to_event(1))\n",
        "                #z_cur[:,1,:] = z_tt1\n",
        "                z_cur = torch.cat([z_cur[:, :1, :], z_tt1.unsqueeze(1), z_cur[:, 2:, :]], dim=1)\n",
        "\n",
        "                if t >= d:\n",
        "                    for j in range(2,d):\n",
        "                        z_tj_prev = z_prev[:,j-1,:].squeeze()\n",
        "                        z_tj_loc, z_tj_scale = self.trans(z_tj_prev)\n",
        "                        z_tj_dist = dist.Normal(z_tj_loc, z_tj_scale)\n",
        "                        with pyro.poutine.scale(scale=self.beta_d):\n",
        "                            z_tj = pyro.sample(f\"z_{t}_{t-j}\", z_tj_dist.to_event(1))\n",
        "\n",
        "                        z_tj_r = pyro.sample(f\"z_{t}_{t-j}_r\", z_tj_dist.to_event(1))\n",
        "                        #z_cur[:,j,:] = z_tj_r\n",
        "                        z_cur = torch.cat([z_cur[:, :j, :], z_tj_r.unsqueeze(1), z_cur[:, j+1:, :]], dim=1)\n",
        "                    z_td_prev = z_prev[:,d-1,:].squeeze()\n",
        "                    z_td_loc, z_td_scale = self.trans(z_td_prev)\n",
        "                    z_td_dist = dist.Normal(z_td_loc, z_td_scale)\n",
        "                    with pyro.poutine.scale(scale=self.beta_d):\n",
        "                        z_td = pyro.sample(f\"z_{t}_{t-d}\", z_td_dist.to_event(1))\n",
        "\n",
        "                if t > 1 and t < d:\n",
        "                    for j in range(2,t+1):\n",
        "                        z_tj_prev = z_prev[:,j-1,:].squeeze()\n",
        "                        z_tj_loc, z_tj_scale = self.trans(z_tj_prev)\n",
        "                        z_tj_dist = dist.Normal(z_tj_loc, z_tj_scale)\n",
        "                        with pyro.poutine.scale(scale=self.beta_d):\n",
        "                            z_tj = pyro.sample(f\"z_{t}_{t-j}\", z_tj_dist.to_event(1))\n",
        "\n",
        "                        z_tj_r = pyro.sample(f\"z_{t}_{t-j}_r\", z_tj_dist.to_event(1))\n",
        "                        #z_cur[:,j,:] = z_tj_r\n",
        "                        z_cur = torch.cat([z_cur[:, :j, :], z_tj_r.unsqueeze(1), z_cur[:, j+1:, :]], dim=1)\n",
        "\n",
        "                # update time step\n",
        "                z_prev = z_cur\n",
        "\n",
        "    def guide_lo(self, mini_batch_x, mini_batch_y):\n",
        "        pyro.module(\"dmm\", self)\n",
        "        T_max = mini_batch_x.size(1) # T\n",
        "        d = self.multisteps # d=1 by default\n",
        "        batch_size = mini_batch_x.size(0) # batch size\n",
        "        mini_batch_combined = torch.cat((mini_batch_x, mini_batch_y), dim=-1)\n",
        "        h_0_contig = self.h_0.expand(1, batch_size, self.rnn.hidden_size).contiguous()\n",
        "        mini_batch_reversed = torch.flip(mini_batch_combined, dims=[1])\n",
        "        # We are using ST-L, leaving no need for reversing RNN.\n",
        "        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)\n",
        "        rnn_output = torch.flip(rnn_output, dims=[1])\n",
        "\n",
        "        # p(z_0)\n",
        "        z_0_expand = self.z_q_0.expand(batch_size, self.z_q_0.size(0))\n",
        "        z_prev = torch.zeros(batch_size, d, self.z_q_0.size(0))\n",
        "        z_prev = torch.cat([z_0_expand.unsqueeze(1), z_prev[:, 1:, :]], dim=1)\n",
        "        z_cur = torch.zeros(batch_size, d, self.z_0.size(0))\n",
        "\n",
        "        # p(z_0)\n",
        "        z_0_batch = self.z_0.expand(batch_size, self.z_0.size(0))\n",
        "\n",
        "        with pyro.plate(\"z_minibatch\", batch_size):\n",
        "            for t in pyro.markov(range(1, T_max + 1)):\n",
        "                # z_{t|t}\n",
        "                z_tt_prev = z_prev[:,0,:].squeeze()\n",
        "                z_tt_loc, z_tt_scale = self.combiner(z_tt_prev, rnn_output[:, t - 1, :])\n",
        "                z_tt_dist = dist.Normal(z_tt_loc, z_tt_scale)\n",
        "                #with pyro.poutine.scale(scale=self.beta_d):\n",
        "                z_tt = pyro.sample(f\"z_{t}_{t}\", z_tt_dist.to_event(1))\n",
        "                #z_cur[:,0,:] = z_tt\n",
        "                z_cur = torch.cat([z_tt.unsqueeze(1), z_cur[:, 1:, :]], dim=1)\n",
        "                # z_{t|t-1}\n",
        "                if t == 1:\n",
        "                    z_tt1_loc, z_tt1_scale = self.trans(z_0_batch)\n",
        "                else:\n",
        "                    z_tt1_loc, z_tt1_scale = self.trans(z_tt_prev)\n",
        "                z_tt1_dist = dist.Normal(z_tt1_loc, z_tt1_scale)\n",
        "\n",
        "                z_tt1 = pyro.sample(f\"z_{t}_{t-1}_r\", z_tt1_dist.to_event(1))\n",
        "                #z_cur[:,1,:] = z_tt1\n",
        "                z_cur = torch.cat([z_cur[:, :1, :], z_tt1.unsqueeze(1), z_cur[:, 2:, :]], dim=1)\n",
        "\n",
        "                if t >= d:\n",
        "                    for j in range(2,d):\n",
        "                        with pyro.poutine.scale(scale=self.beta_d):\n",
        "                            z_tj = pyro.sample(f\"z_{t}_{t-j}\", z_tt_dist.to_event(1))\n",
        "                        z_tj_prev = z_prev[:,j-1,:].squeeze()\n",
        "                        z_tj_loc, z_tj_scale = self.trans(z_tj_prev)\n",
        "                        z_tj_dist = dist.Normal(z_tj_loc, z_tj_scale)\n",
        "\n",
        "                        z_tj_r = pyro.sample(f\"z_{t}_{t-j}_r\", z_tj_dist.to_event(1))\n",
        "                        #z_cur[:,j,:] = z_tj_r\n",
        "                        z_cur = torch.cat([z_cur[:, :j, :], z_tj_r.unsqueeze(1), z_cur[:, j+1:, :]], dim=1)\n",
        "                    with pyro.poutine.scale(scale=self.beta_d):\n",
        "                        z_td = pyro.sample(f\"z_{t}_{t-d}\", z_tt_dist.to_event(1))\n",
        "\n",
        "                if t > 1 and t < d:\n",
        "                    for j in range(2,t+1):\n",
        "                        with pyro.poutine.scale(scale=self.beta_d):\n",
        "                            z_tj = pyro.sample(f\"z_{t}_{t-j}\", z_tt_dist.to_event(1))\n",
        "                        z_tj_prev = z_prev[:,j-1,:].squeeze()\n",
        "                        z_tj_loc, z_tj_scale = self.trans(z_tj_prev)\n",
        "                        z_tj_dist = dist.Normal(z_tj_loc, z_tj_scale)\n",
        "\n",
        "                        z_tj_r = pyro.sample(f\"z_{t}_{t-j}_r\", z_tj_dist.to_event(1))\n",
        "                        #z_cur[:,j,:] = z_tj_r\n",
        "                        z_cur = torch.cat([z_cur[:, :j, :], z_tj_r.unsqueeze(1), z_cur[:, j+1:, :]], dim=1)\n",
        "\n",
        "                # update time step\n",
        "                z_prev = z_cur\n",
        "\n",
        "\n",
        "def elbo_lo(model, guide, *args, **kwargs):\n",
        "    # Get the trace from the guide and the model\n",
        "    guide_trace = pyro.poutine.trace(guide).get_trace(*args, **kwargs)\n",
        "    model_trace = pyro.poutine.trace(pyro.poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)\n",
        "\n",
        "    elbo = 0.0\n",
        "\n",
        "    # Iterate through the nodes in the guide trace\n",
        "    for name, site in guide_trace.nodes.items():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            log_prob = site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "            # Check if the variable name contains \"_r\" and exclude it from the ELBO calculation\n",
        "            if \"_r\" not in name:\n",
        "                elbo += log_prob\n",
        "\n",
        "    # Iterate through the nodes in the model trace\n",
        "    for name, site in model_trace.nodes.items():\n",
        "        if site[\"type\"] == \"sample\":\n",
        "            log_prob = site[\"fn\"].log_prob(site[\"value\"]).sum()\n",
        "            if \"_r\" not in name:\n",
        "                elbo -= log_prob\n",
        "\n",
        "    return elbo\n",
        "\n",
        "# find emperical mean of a transformed distribution\n",
        "def mean_iaf(z_loc, z_scale, iaf_module, num_samples = 1000):\n",
        "    base_dist = dist.Normal(z_loc, z_scale).to_event(1)\n",
        "    transformed_dist = TransformedDistribution(base_dist, iaf_module)\n",
        "    samples = transformed_dist.sample([num_samples])\n",
        "    estimated_mean = samples.mean(dim=0)\n",
        "    return estimated_mean\n",
        "\n",
        "\n",
        "\n",
        "# draw visualization for y\n",
        "def visualize(dmm_trained, x_data, y_data, data_idx, y_idx, find_RMSE=False, store_pic=False):\n",
        "    if not find_RMSE:\n",
        "        x_obs = x_data[data_idx:data_idx+1,:,:]\n",
        "        y_obs = y_data[data_idx:data_idx+1,:,:]\n",
        "        batch_size = 1\n",
        "    else:\n",
        "        x_obs = x_data\n",
        "        y_obs = y_data\n",
        "        batch_size = x_data.shape[0]\n",
        "        N = batch_size\n",
        "    y_pred = []\n",
        "    y_scales = []\n",
        "    x_pred = []\n",
        "    x_scales = []\n",
        "    z_pred = []\n",
        "    z_scales = []\n",
        "    T = x_obs.shape[1]\n",
        "\n",
        "    obs = torch.cat((x_obs, y_obs), dim=-1)\n",
        "    h_0_contig = dmm_trained.h_0.expand(1, batch_size, dmm_trained.rnn.hidden_size).contiguous()\n",
        "    obs_reversed = torch.flip(obs, dims=[1])\n",
        "    rnn_output, _ = dmm_trained.rnn(obs_reversed, h_0_contig)\n",
        "    rnn_output = torch.flip(rnn_output, dims=[1])\n",
        "    z_prev = dmm_trained.z_q_0\n",
        "    for t in range(T):\n",
        "        z_t_loc, z_t_scale = dmm_trained.combiner(z_prev, rnn_output[:, t, :])\n",
        "        if len(dmm_trained.iafs) > 0:\n",
        "            z_t_loc = mean_iaf(z_t_loc, z_t_scale, dmm_trained.iafs, num_samples = 1000)\n",
        "        # naive model\n",
        "        if dmm_trained.zx_dim == 0 and dmm_trained.zy_dim == 0:\n",
        "            x_loc, x_scale = dmm_trained.emitter_z_x(z_t_loc)\n",
        "            y_loc, y_scale = dmm_trained.emitter_z_y(z_t_loc)\n",
        "        # cca\n",
        "        if dmm_trained.zx_dim != 0 and dmm_trained.zy_dim != 0:\n",
        "            z_t = z_t_loc[:, :dmm_trained.z_dim]\n",
        "            zx_t = z_t_loc[:, dmm_trained.z_dim:dmm_trained.z_dim+dmm_trained.zx_dim]\n",
        "            zy_t = z_t_loc[:, dmm_trained.z_dim+dmm_trained.zx_dim:]\n",
        "            x_loc, x_scale = dmm_trained.emitter_zx_x(z_t, zx_t)\n",
        "            y_loc, y_scale = dmm_trained.emitter_zy_y(z_t, zy_t)\n",
        "        # pls\n",
        "        if dmm_trained.zx_dim != 0 and dmm_trained.zy_dim == 0:\n",
        "            z_t = z_t_loc[:, :dmm_trained.z_dim]\n",
        "            zx_t = z_t_loc[:, dmm_trained.z_dim:]\n",
        "            x_loc, x_scale = dmm_trained.emitter_zx_x(z_t, zx_t)\n",
        "            y_loc, y_scale = dmm_trained.emitter_z_y(z_t)\n",
        "        y_pred.append(y_loc.squeeze(0))\n",
        "        y_scales.append(y_scale.squeeze(0))\n",
        "        x_pred.append(x_loc.squeeze(0))\n",
        "        x_scales.append(x_scale.squeeze(0))\n",
        "        z_pred.append(z_t_loc.squeeze(0))\n",
        "        z_scales.append(z_t_scale.squeeze(0))\n",
        "        z_prev = z_t_loc\n",
        "    if find_RMSE:\n",
        "        y_pred = torch.stack(y_pred, dim=1).detach().numpy()\n",
        "        y_scales = torch.stack(y_scales, dim=1).detach().numpy()\n",
        "        x_pred = torch.stack(x_pred, dim=1).detach().numpy()\n",
        "        x_scales = torch.stack(x_scales, dim=1).detach().numpy()\n",
        "        z_pred = torch.stack(z_pred, dim=1).detach().numpy()\n",
        "        z_scales = torch.stack(z_scales, dim=1).detach().numpy()\n",
        "    else:\n",
        "        y_pred = torch.stack(y_pred).detach().numpy()\n",
        "        y_scales = torch.stack(y_scales).detach().numpy()\n",
        "        x_pred = torch.stack(x_pred).detach().numpy()\n",
        "        x_scales = torch.stack(x_scales).detach().numpy()\n",
        "        z_pred = torch.stack(z_pred).detach().numpy()\n",
        "        z_scales = torch.stack(z_scales).detach().numpy() #print(y_pred.shape)\n",
        "    # upper and lower CI\n",
        "    y_upper = y_pred + 2*y_scales\n",
        "    y_lower = y_pred - 2*y_scales\n",
        "    x_upper = x_pred + 2*x_scales\n",
        "    x_lower = x_pred - 2*x_scales\n",
        "    z_upper = z_pred + 2*z_scales\n",
        "    z_lower = z_pred - 2*z_scales\n",
        "    # calculate RMSE:\n",
        "    if find_RMSE:\n",
        "        squared_diff_x = (x_data - x_pred) ** 2\n",
        "        squared_diff_y = (y_data - y_pred) ** 2\n",
        "        MSE = (squared_diff_x.sum() + squared_diff_y.sum()) / (N * T)\n",
        "        RMSE = torch.sqrt(MSE)\n",
        "        # save all the predictions:\n",
        "        if store_pic:\n",
        "            x_dim = x_upper.shape[-1]\n",
        "            y_dim = y_upper.shape[-1]\n",
        "            z_dim = z_upper.shape[-1]\n",
        "\n",
        "            for xj in range(x_dim):\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(x_pred[data_idx, :, xj], label='Prediction')\n",
        "                plt.plot(x_data[data_idx, :, xj], label='Observations')\n",
        "                if len(dmm_trained.iafs) == 0:\n",
        "                    plt.fill_between(range(len(x_upper[data_idx, :, xj])), x_lower[data_idx,:,xj], x_upper[data_idx, :, xj], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "                plt.xlabel(\"Time Step\")\n",
        "                plt.ylabel(\"Value\")\n",
        "                plt.legend()\n",
        "                filename = f'x_{data_idx}_{xj+1}.png'\n",
        "                plt.savefig(filename)\n",
        "                plt.close()\n",
        "                files.download(filename)\n",
        "                print(filename)\n",
        "\n",
        "            for yj in range(y_dim):\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(y_pred[data_idx, :, yj], label='Prediction')\n",
        "                plt.plot(y_data[data_idx, :, yj], label='Observations')\n",
        "                if len(dmm_trained.iafs) == 0:\n",
        "                    plt.fill_between(range(len(y_upper[data_idx, :, yj])), y_lower[data_idx,:,yj], y_upper[data_idx, :, yj], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "                plt.xlabel(\"Time Step\")\n",
        "                plt.ylabel(\"Value\")\n",
        "                plt.legend()\n",
        "                filename = f'y_{data_idx}_{yj+1}.png'\n",
        "                plt.savefig(filename)\n",
        "                plt.close()\n",
        "                files.download(filename)\n",
        "                print(filename)\n",
        "\n",
        "            for zj in range(z_dim):\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.plot(z_pred[data_idx, :, zj], label='Prediction')\n",
        "                if len(dmm_trained.iafs) == 0:\n",
        "                    plt.fill_between(range(len(z_upper[data_idx, :, zj])), z_lower[data_idx,:,zj], z_upper[data_idx, :, zj], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "                plt.xlabel(\"Time Step\")\n",
        "                plt.ylabel(\"Value\")\n",
        "                plt.legend()\n",
        "                filename = f'z_{data_idx}_{zj+1}.png'\n",
        "                plt.savefig(filename)\n",
        "                plt.close()\n",
        "                files.download(filename)\n",
        "                print(filename)\n",
        "        return RMSE\n",
        "    else:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(y_pred[:, y_idx], label='Prediction')\n",
        "        plt.plot(y_data[data_idx, :, y_idx], label='Actual Values')\n",
        "        # Fill the area between y_upper and y_lower\n",
        "        if len(dmm_trained.iafs) == 0:\n",
        "            plt.fill_between(range(len(y_upper[:, y_idx])), y_lower[:, y_idx], y_upper[:, y_idx], color='gray', alpha=0.2, label='Confidence Interval')\n",
        "        plt.title(\"Visualization of the First Dimension over 100 Time Steps\")\n",
        "        plt.xlabel(\"Time Step\")\n",
        "        plt.ylabel(\"Value\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Training one epoch of the training set\n",
        "def train(svi, train_loader):\n",
        "    epoch_loss = 0.\n",
        "    for x, y in train_loader: # x is mini-batch\n",
        "        epoch_loss += svi.step(x,y)\n",
        "\n",
        "    # return average epoch loss\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_loss / normalizer_train\n",
        "    return total_epoch_loss_train\n",
        "\n",
        "# evaluate model on test set\n",
        "def evaluate(svi, test_loader):\n",
        "    test_loss = 0.\n",
        "    for x, y in test_loader:\n",
        "        test_loss += svi.evaluate_loss(x,y)\n",
        "    normalizer_test = len(test_loader.dataset)\n",
        "    total_epoch_loss_test = test_loss / normalizer_test\n",
        "    return total_epoch_loss_test\n",
        "\n",
        "def train_KL_annealing(svi, train_loader, epoch, annealing_epochs, minimum_annealing_factor):\n",
        "    batch_size = train_loader.batch_size\n",
        "    N_mini_batches = len(train_loader)\n",
        "    epoch_nll = 0.0\n",
        "    for which_mini_batch, (x, y) in enumerate(train_loader):\n",
        "        if annealing_epochs > 0 and epoch < annealing_epochs:\n",
        "            annealing_factor = minimum_annealing_factor + (1.0 - minimum_annealing_factor) * (\n",
        "                float(which_mini_batch + epoch * N_mini_batches + 1)\n",
        "                / float(annealing_epochs * N_mini_batches)\n",
        "            )\n",
        "        else:\n",
        "            annealing_factor = 1.0\n",
        "        epoch_nll += svi.step(x, y, annealing_factor)\n",
        "    # return average epoch loss\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = epoch_nll / normalizer_train\n",
        "    return total_epoch_loss_train\n"
      ],
      "metadata": {
        "id": "-wWpIObugW7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation Data"
      ],
      "metadata": {
        "id": "JSPzlN9-ggOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(torch.initial_seed() % (2**32 - 1))\n",
        "\n",
        "def nonlinear_transition(z_prev, A, nonlinearity, noise_std):\n",
        "    z_linear = np.dot(A, z_prev)\n",
        "    z_nonlinear = 0.5 * z_linear + 0.5 * nonlinearity(z_linear)\n",
        "    z_next = z_nonlinear + np.random.normal(0, noise_std, z_prev.shape)\n",
        "    return z_next\n",
        "\n",
        "def generate_system(T, A0, Ax, Ay, Wx, Wy, initial_states, noise_std=0.1, epsilon_x=0.05, epsilon_y=0.05):\n",
        "    Z0 = np.zeros(T+1)\n",
        "    Zx = np.zeros((T+1, Ax.shape[0]))\n",
        "    Zy = np.zeros((T+1, Ay.shape[0]))\n",
        "    X = np.zeros((T, Wx.shape[0]))\n",
        "    Y = np.zeros((T, Wy.shape[0]))\n",
        "\n",
        "    Z0[0] = initial_states['z0']\n",
        "    Zx[0, :], Zy[0, :] = initial_states['zx'], initial_states['zy']\n",
        "\n",
        "    for t in range(1,T+1):\n",
        "        Z0[t] = nonlinear_transition(Z0[t-1], A0, np.sin, noise_std)\n",
        "        Zx[t, :] = nonlinear_transition(Zx[t-1, :], Ax, np.tanh, noise_std)\n",
        "        Zy[t, :] = nonlinear_transition(Zy[t-1, :], Ay, np.tanh, noise_std)\n",
        "\n",
        "        x_state = np.concatenate(([Z0[t]], Zx[t, :]))\n",
        "        y_state = np.concatenate(([Z0[t]], Zy[t, :]))\n",
        "        X[t-1, :] = np.dot(Wx, x_state) + np.random.normal(0, epsilon_x, Wx.shape[0])\n",
        "        Y[t-1, :] = np.dot(Wy, y_state) + np.random.normal(0, epsilon_y, Wy.shape[0])\n",
        "\n",
        "    return Z0, Zx, Zy, X, Y\n",
        "\n",
        "def generate_multiple_samples(N, T, A0, Ax, Ay, Wx, Wy, noise_std, epsilon_x, epsilon_y):\n",
        "    data_x = np.zeros((N, T, Wx.shape[0]))\n",
        "    data_y = np.zeros((N, T, Wy.shape[0]))\n",
        "\n",
        "    for n in range(N):\n",
        "        initial_states = {\n",
        "            'z0': np.random.rand(1),\n",
        "            'zx': np.random.rand(3),\n",
        "            'zy': np.random.rand(2)\n",
        "        }\n",
        "        _, _, _, X, Y = generate_system(T, A0, Ax, Ay, Wx, Wy, initial_states, noise_std, epsilon_x, epsilon_y)\n",
        "        data_x[n] = X\n",
        "        data_y[n] = Y\n",
        "\n",
        "    return data_x, data_y\n",
        "\n",
        "# Parameters\n",
        "N = 500\n",
        "T = 100\n",
        "A0 = np.array([[0.95]])\n",
        "Ax = np.array([[0.9, 0.1, 0.0], [0.1, 0.8, 0.1], [0.0, 0.1, 0.9]])\n",
        "Ay = np.array([[0.85, 0.15], [0.15, 0.85]])\n",
        "Wx = np.random.rand(10, 4)  # Observation matrix for x\n",
        "Wy = np.random.rand(5, 3)  # Observation matrix for y\n",
        "\n",
        "\n",
        "# Generate N samples\n",
        "data_x, data_y = generate_multiple_samples(N, T, A0, Ax, Ay, Wx, Wy, 0.1, 0.05, 0.05)\n",
        "data_x = torch.tensor(data_x, dtype=torch.float32)\n",
        "data_y = torch.tensor(data_y, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(data_x, data_y)\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 10\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "AlDKAgnmgjia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Financial Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "FasTQld1g6Hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/xxxxx.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "data_X = df.iloc[:, 1:11].astype(float).values\n",
        "data_Y = df.iloc[:, 11:21].astype(float).values\n",
        "\n",
        "# Normalize the data up front\n",
        "mean_X = data_X.mean(axis=0)\n",
        "std_X = data_X.std(axis=0)\n",
        "normalized_X = (data_X - mean_X) / std_X\n",
        "\n",
        "mean_Y = data_Y.mean(axis=0)\n",
        "std_Y = data_Y.std(axis=0)\n",
        "normalized_Y = (data_Y - mean_Y) / std_Y\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 30\n",
        "sequences_X = []\n",
        "sequences_Y = []\n",
        "for i in range(len(data_X) - sequence_length + 1):\n",
        "    sequences_X.append(normalized_X[i:i+sequence_length])\n",
        "    sequences_Y.append(normalized_Y[i:i+sequence_length])\n",
        "\n",
        "sequences_X = np.array(sequences_X)\n",
        "sequences_Y = np.array(sequences_Y)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "tensor_X = torch.tensor(sequences_X, dtype=torch.float32)\n",
        "tensor_Y = torch.tensor(sequences_Y, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "dataset = TensorDataset(tensor_X, tensor_Y)\n",
        "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)"
      ],
      "metadata": {
        "id": "ExPP0PBPg_bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage"
      ],
      "metadata": {
        "id": "S1RS6bXrgnJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# experiment x only.\n",
        "\n",
        "my_model = D2PCCA(\n",
        "    ## naive\n",
        "    x_dim=10,\n",
        "    y_dim=10,\n",
        "    z_dim=1,\n",
        "    emission_dim_z_x=20,\n",
        "    emission_dim_z_y=20,\n",
        "    transition_dim_z=20,\n",
        "    rnn_dim = 30,\n",
        "    rnn_dropout_rate=.1,\n",
        "    ## cca\n",
        "    zx_dim=2,\n",
        "    zy_dim=2, #2\n",
        "    emission_dim_zx_x=20,\n",
        "    emission_dim_zy_y=20,\n",
        "    transition_dim_zx=10,\n",
        "    transition_dim_zy=10,\n",
        "    multisteps=2,\n",
        "    # iafs\n",
        "    #num_iafs=5,\n",
        "    #iaf_dim=10,\n",
        "    #beta_d = .01\n",
        ")\n",
        "\n",
        "# setup optimizer\n",
        "adam_params = {\n",
        "    \"lr\": 0.0003,\n",
        "    \"betas\": (0.96, 0.999),\n",
        "    \"clip_norm\": 20.0,\n",
        "    \"lrd\": 0.99996,\n",
        "    \"weight_decay\": 2.0,\n",
        "}\n",
        "\n",
        "# clear parameters\n",
        "pyro.clear_param_store()\n",
        "\n",
        "adam = ClippedAdam(adam_params)\n",
        "\n",
        "# D2PCCA\n",
        "svi = SVI(my_model.model, my_model.guide, adam, Trace_ELBO())\n",
        "# with Latent Overshooting\n",
        "# svi = SVI(my_model.model_lo, my_model.guide_lo, adam, elbo_lo)"
      ],
      "metadata": {
        "id": "8__VZEc4hIUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 1500\n",
        "\n",
        "# For KL-Annealing Training\n",
        "#annealing_epochs = 90\n",
        "#minimum_annealing_factor = 0.2\n",
        "\n",
        "train_elbo = []\n",
        "\n",
        "# training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    total_epoch_loss_train = train(svi, train_loader)\n",
        "    # total_epoch_loss_train = train_KL_annealing(svi, train_loader, epoch, annealing_epochs, minimum_annealing_factor)\n",
        "    train_elbo.append(-total_epoch_loss_train)\n",
        "    print(\"[epoch %03d]  average training loss: %.4f\" % (epoch, total_epoch_loss_train))\n",
        "    if epoch % 50 == 0:\n",
        "        rmse = visualize(my_model, tensor_X, tensor_Y, 10, 3, True, False)\n",
        "        print(\"[epoch %03d]  RMESE: %.4f\" % (epoch, rmse))\n"
      ],
      "metadata": {
        "id": "4Hx8CAYriGdF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}